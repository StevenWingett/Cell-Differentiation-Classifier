{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ed1befa-b844-4ea5-b8b0-3eb84ae04a24",
   "metadata": {},
   "source": [
    "# Assess predictions\n",
    "\n",
    "Evaluate how good the predictions made the logistic regression classifier actually where.\n",
    "\n",
    "Takes as input:\n",
    "1. Classifier results file: *.predicted_summary.tsv\n",
    "2. Input metadata file 'dataset_summary.tsv'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb0201d-f294-4785-8e5e-27f697b41dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1420b276-ea62-41cc-9ded-e23782c370ca",
   "metadata": {},
   "source": [
    "## Setup (edit as required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9809e7-ad4c-4a97-8b2a-ffb5fff4f3b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup (edit as required)\n",
    "classifier_results_file = 'retention_group_1_2_log2.predicted_summary.tsv.gz'\n",
    "metadata_file = 'dataset_summary.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4827c3e-70ab-46f2-b94d-801b693447ef",
   "metadata": {},
   "source": [
    "## Read in results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af4ded-ed44-4829-8cf4-fae070b8e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in results\n",
    "print(\"Reading in classifier results: \" + classifier_results_file)\n",
    "classifier_results = pd.read_csv(classifier_results_file, sep=\"\\t\")\n",
    "\n",
    "print(\"Reading in datasets metadata: \" + metadata_file)\n",
    "metadata = pd.read_csv(metadata_file, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60335c1-bc52-465f-95fc-8685f8098b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single data structure\n",
    "metadata = metadata.loc[:, ['Accession', 'Cell_line', 'Jerber_model_score', 'Diff_efficiency', 'Result']]\n",
    "\n",
    "metadata = metadata.rename(columns={'Accession' : 'accession',\n",
    "                                    'Cell_line' : 'cell_line',\n",
    "                                    'Jerber_model_score' : 'jerber_model_score',\n",
    "                                    'Diff_efficiency' : 'experimental_diff_efficiency',\n",
    "                                    'Result' : 'experimental_differentiated'\n",
    "                                   })\n",
    "\n",
    "metadata['experimental_differentiated'] = np.where(metadata['experimental_differentiated'] == 'succeeded', 1, 0)\n",
    "\n",
    "classifier_results = classifier_results.rename(columns={'differentiated' : 'predicted_differentiated'})\n",
    "\n",
    "classifier_results['p_coeff_variation'] = classifier_results['p_stdp'] / classifier_results['p_average']\n",
    "\n",
    "classifier_results = classifier_results.loc[:, ['accession', 'p_average', 'p_stdp', 'p_coeff_variation', 'predicted_differentiated']]  #Nicer order\n",
    "                                                  \n",
    "classifier_results = pd.merge(metadata, classifier_results, how='right', on='accession')\n",
    "\n",
    "classifier_results = classifier_results.sort_values(by=['cell_line', 'p_average'])\n",
    "\n",
    "classifier_results['correct_prediction'] = np.where(classifier_results['experimental_differentiated'] == classifier_results['predicted_differentiated'], 1, 0)\n",
    "\n",
    "classifier_results = classifier_results.reset_index(drop=True)\n",
    "\n",
    "del(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2718e763-1fe2-4b3d-b632-c00a9d08b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for consistency in within cell lines (concordancy of classifier predictions)\n",
    "consistency_data = classifier_results.loc[:, ['cell_line', 'predicted_differentiated']]\n",
    "\n",
    "print(f\"Number of accessions: {consistency_data.shape[0]}\")\n",
    "print(f\"Number of different cell lines: {consistency_data['cell_line'].drop_duplicates().shape[0]}\")\n",
    "\n",
    "boolean_to_select = consistency_data.duplicated(subset='cell_line', keep=False)   #Identfy cell-lines present more than once\n",
    "consistency_data = consistency_data[boolean_to_select]\n",
    "\n",
    "print(f\"Total number of accessions which are part of a replicated cell line: {consistency_data['cell_line'].shape[0]}\")\n",
    "\n",
    "\n",
    "replicate_groups = consistency_data['cell_line'].drop_duplicates().shape[0]\n",
    "print(f'Number of replicated cell lines (i.e. replicate groups): {replicate_groups}')\n",
    "\n",
    "consistency_data = consistency_data.drop_duplicates()\n",
    "\n",
    "concordant_cell_lines = (consistency_data\n",
    "                             .duplicated(subset='cell_line', keep=False)\n",
    "                             .value_counts()\n",
    "                        )\n",
    "\n",
    "concordant_cell_lines = concordant_cell_lines[False]\n",
    "discordant_cell_lines = replicate_groups - concordant_cell_lines\n",
    "\n",
    "print(f'Concordant cell lines: {concordant_cell_lines}')\n",
    "print(f'Discordant cell lines: {discordant_cell_lines}')\n",
    "print(f'%Concordancy: {round(100 * concordant_cell_lines / replicate_groups, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b473edb-7427-47ff-85bd-a2b80c2d5ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "cm = confusion_matrix(classifier_results['experimental_differentiated'], classifier_results['predicted_differentiated'])\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax, cmap=plt.cm.Blues, fmt='g')\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(['Undifferentiated', 'Differentiated'])\n",
    "ax.yaxis.set_ticklabels(['Undifferentiated', 'Differentiated'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b96aca-c909-44cb-8f76-d866c7fd29af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "acc = accuracy_score(classifier_results['experimental_differentiated'], classifier_results['predicted_differentiated'])\n",
    "print('Accuracy: ', round(acc, 3))\n",
    "print('==================================================================================')\n",
    "\n",
    "# Calculate Cohen's Kappa score\n",
    "cka = cohen_kappa_score(classifier_results['experimental_differentiated'], classifier_results['predicted_differentiated'])\n",
    "print(\"Cohen's Kappa\")\n",
    "print()\n",
    "print(\"Cohen suggested the Kappa result be interpreted as follows: \")\n",
    "print(\"values ≤ 0 as indicating no agreement\\n0.01–0.20 as none to slight\\n0.21–0.40 as fair\")\n",
    "print(\"0.41– 0.60 as moderate\\n0.61–0.80 as substantial\\n0.81–1.00 as almost perfect agreement.\\n\")\n",
    "print('Cohen\\'s Kappa: ', round(cka, 3))\n",
    "print('==================================================================================')\n",
    "\n",
    "# F1 score\n",
    "f1 = f1_score(classifier_results['experimental_differentiated'], classifier_results['predicted_differentiated'])\n",
    "print('F1 Score')\n",
    "print()\n",
    "print('F1 = 2 * (precision * recall) / (precision + recall)')\n",
    "print('A model will obtain a high F1 score if both Precision and Recall are high')\n",
    "print('A model will obtain a low F1 score if both Precision and Recall are low')\n",
    "print('A model will obtain a medium F1 score if one of Precision and Recall is low and the other is high')\n",
    "print()\n",
    "print(f'F1 score: {round(f1, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3e4bd5-72dc-413b-b9b8-f24709f2a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a precision recall curve\n",
    "column_names = ['Precision', 'Recall']\n",
    "precision_recall = pd.DataFrame(columns=column_names)\n",
    "\n",
    "\n",
    "p_value_thresholds = (classifier_results\n",
    "                        .loc[:, 'p_average']\n",
    "                        .drop_duplicates()\n",
    "                        .sort_values()\n",
    "                        .reset_index(drop=True)\n",
    "                        .iloc[0:-1]    #Remove last value since nothing larger than this\n",
    "                     )\n",
    "\n",
    "                        \n",
    "for p_value_threshold in p_value_thresholds:\n",
    "    threshold_specific_prediction = np.where(classifier_results['p_average'] > p_value_threshold, 1, 0)\n",
    "\n",
    "    precision = precision_score(classifier_results['experimental_differentiated'], threshold_specific_prediction)\n",
    "    recall = recall_score(classifier_results['experimental_differentiated'], threshold_specific_prediction)\n",
    "\n",
    "    precision_recall_current = pd.DataFrame([[precision, recall]], \n",
    "                                            columns=column_names\n",
    "                                           )\n",
    "    precision_recall = pd.concat([precision_recall, precision_recall_current],\n",
    "                                ignore_index=True)\n",
    "\n",
    "    \n",
    "# Plot results\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.lineplot(data=precision_recall, x='Recall', y='Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlim(0, 1.05)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.lineplot(data=precision_recall, x='Recall', y='Precision')\n",
    "plt.title('Precision-Recall Curve (autoscale)')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8b1cae-09a0-4012-8f90-109f5de3ac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine recall value with 100% precision\n",
    "if precision_recall['Precision'].max() == 1:\n",
    "    best_recall = (precision_recall\n",
    "                       .query('Precision == 1')\n",
    "                       .loc[:, 'Recall']\n",
    "                       .max()\n",
    "                  )\n",
    "else:\n",
    "    print('Precision never reaches 1')\n",
    "    \n",
    "print(f'100% Precision with Recall of {round(best_recall * 100, 1)}%')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de617e2-ace8-4d43-b776-93b4351a5ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot predicted vs expected\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.scatterplot(x='experimental_diff_efficiency', \n",
    "            y='p_average',\n",
    "            hue='experimental_differentiated',\n",
    "            style='correct_prediction',\n",
    "            data=classifier_results,\n",
    "            alpha=0.7)\n",
    "\n",
    "plt.title('Classifier average p-value vs differentiation efficiency')\n",
    "plt.xlabel('Experimental differentiation efficiency')\n",
    "plt.ylabel('Classifier p value')\n",
    "plt.axhline(0.5, color='r', linestyle='--')\n",
    "plt.axvline(0.2, color='r', linestyle='--')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Put the legend out of the figure\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39ea027-ea91-42a1-9622-d3bbb9838f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show p-value std of variance vs p-value average\n",
    "plt.figure(figsize=(10 ,10))\n",
    "sns.scatterplot(x='p_stdp', \n",
    "            y='p_average',\n",
    "            hue='experimental_differentiated',\n",
    "            style='correct_prediction',\n",
    "            data=classifier_results,\n",
    "            alpha=0.7)\n",
    "\n",
    "plt.title('How classifier prediction success varies with p-value and p-value variation I')\n",
    "plt.xlabel('Classifier standard deviation of p-value')\n",
    "plt.ylabel('Classifier average p-value')\n",
    "plt.axhline(0.5, color='r', linestyle='--')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Put the legend out of the figure\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0de94b-f263-4e54-958b-9a357e1fc4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show p-value coefficient of variance vs p-value average\n",
    "plt.figure(figsize=(10 ,10))\n",
    "sns.scatterplot(x='p_coeff_variation', \n",
    "            y='p_average',\n",
    "            hue='experimental_differentiated',\n",
    "            style='correct_prediction',\n",
    "            data=classifier_results,\n",
    "            alpha=0.7)\n",
    "\n",
    "plt.title('How classifier prediction success varies with p-value and p-value variation II')\n",
    "plt.xlabel('Classifier coefficient of variance of p-value')\n",
    "plt.ylabel('Classifier average p-value')\n",
    "plt.axhline(0.5, color='r', linestyle='--')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Put the legend out of the figure\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08ab6e8-627e-4407-a488-357d8f38683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter_3d(classifier_results, \n",
    "                    x='experimental_diff_efficiency', \n",
    "                    y='p_average', \n",
    "                    z='p_coeff_variation',\n",
    "                    color='correct_prediction',\n",
    "                    opacity=0.7)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f2dfe-5809-4f28-a090-2dc0574e26ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
