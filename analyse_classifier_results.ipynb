{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed821968-cff5-420a-9e52-e2c0c960cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import os.path\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddaeaee-a715-4d10-b6e1-7e13ed0b9317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "# Rest\n",
    "results_file = 'classifier_results.tsv.gz'\n",
    "\n",
    "# Metadata\n",
    "metadata_file = 'dataset_summary.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a2ae44-d1e1-451f-8052-71e12c9933ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "print(\"Reading in classifier results file: \" + results_file)\n",
    "full_results = pd.read_csv(results_file, sep=\"\\t\")\n",
    "print()\n",
    "\n",
    "print(\"Reading in metadata: \" + metadata_file)\n",
    "metadata = pd.read_csv(metadata_file, sep=\"\\t\")\n",
    "print(f'Metadata number of Accessions: {metadata.shape[0]}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb651e2b-fb22-4c51-ba64-7475a5b1f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the mean p-values and standard deviations\n",
    "\n",
    "grouped_results = (full_results\n",
    "                    .loc[:, ['Accession', 'Predicted_p_value']]\n",
    "                    .groupby(by='Accession')\n",
    "                  )\n",
    "\n",
    "mean_p_values = (grouped_results\n",
    "                    .mean()\n",
    "                    .rename(columns={'Predicted_p_value': 'Mean_predicted_p_value'})\n",
    "                )\n",
    "\n",
    "mean_p_values.loc[:, 'Accession'] = mean_p_values.index\n",
    "mean_p_values = mean_p_values.reset_index(drop=True)\n",
    "\n",
    "std_p_values = (grouped_results\n",
    "                    .std(ddof=0)\n",
    "                    .rename(columns={'Predicted_p_value': 'STD_predicted_p_value'})\n",
    "                )\n",
    "\n",
    "std_p_values.loc[:, 'Accession'] = std_p_values.index\n",
    "std_p_values = std_p_values.reset_index(drop=True)\n",
    "\n",
    "\n",
    "results = (full_results\n",
    "               .query('Iteration == 1')\n",
    "               .loc[:, ['Accession', 'Expected']]\n",
    "          )\n",
    "\n",
    "results = pd.merge(results, mean_p_values, how='inner')\n",
    "results = pd.merge(results, std_p_values, how='inner')\n",
    "\n",
    "del(grouped_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a80e60f-8eb7-4836-9568-eb90a9d5fd91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Determine the number of TP, TN, FP, FN\n",
    "p_value_threshold = 0.5\n",
    "results['Predicted'] = np.where(results['Mean_predicted_p_value'] > p_value_threshold, 1, 0)\n",
    "\n",
    "for i, row in results.iterrows():\n",
    "    #print(results.loc[i, 'Expected'] + results.loc[i, 'Predicted'])\n",
    "    if results.loc[i, 'Expected'] == 0:\n",
    "        if results.loc[i, 'Predicted'] == 0:\n",
    "            results.loc[i, 'Accuracy'] = 'TN'\n",
    "        else:\n",
    "            results.loc[i, 'Accuracy'] = 'FP'\n",
    "    else:\n",
    "        if results.loc[i, 'Predicted'] == 0:\n",
    "            results.loc[i, 'Accuracy'] = 'FN'\n",
    "        else: \n",
    "            results.loc[i, 'Accuracy'] = 'TP'\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8036b4d3-c762-4913-96a4-5194d244b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "cm = confusion_matrix(results['Expected'], results['Predicted'])\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax, cmap=plt.cm.Blues, fmt='g')\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(['Undifferentiated', 'Differentiated'])\n",
    "ax.yaxis.set_ticklabels(['Undifferentiated', 'Differentiated'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8702a92-5cac-4132-a457-a30a03ee9726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "acc = accuracy_score(results['Expected'], results['Predicted'])\n",
    "print('Accuracy: ', acc)\n",
    "print()\n",
    "\n",
    "# Calculate Cohen's Kappa score\n",
    "cka = cohen_kappa_score(results['Expected'], results['Predicted'])\n",
    "print('Cohen\\'s Kappa: ', cka)\n",
    "print()\n",
    "print('Cohen suggested the Kappa result be interpreted as follows:')\n",
    "print('values ≤ 0: no agreement')\n",
    "print('0.01 – 0.20: none to slight')\n",
    "print('0.21 – 0.40: fair')\n",
    "print('0.41 – 0.60: moderate')\n",
    "print('0.61 – 0.80: substantial')\n",
    "print('0.81 – 1.00: almost perfect')\n",
    "print()\n",
    "\n",
    "# F1 score\n",
    "f1 = f1_score(results['Expected'], results['Predicted'])\n",
    "print(f'F1 score: {f1}')\n",
    "print()\n",
    "print('F1 = 2 * (precision * recall) / (precision + recall)')\n",
    "print('A model will obtain a high F1 score if both Precision and Recall are high')\n",
    "print('A model will obtain a low F1 score if both Precision and Recall are low')\n",
    "print('A model will obtain a medium F1 score if one of Precision and Recall is low and the other is high')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fa80e3-b0fe-4ba8-9aae-96ae1f02c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a precision recall curve\n",
    "column_names = ['Precision', 'Recall']\n",
    "precision_recall = pd.DataFrame(columns=column_names)\n",
    "\n",
    "\n",
    "p_value_thresholds = (results\n",
    "                        .loc[:, 'Mean_predicted_p_value']\n",
    "                        .drop_duplicates()\n",
    "                        .sort_values()\n",
    "                        .reset_index(drop=True)\n",
    "                        .iloc[0:-1]    #Remove last value since nothing larger than this\n",
    "                     )\n",
    "\n",
    "                        \n",
    "for p_value_threshold in p_value_thresholds:\n",
    "    threshold_specific_prediction = np.where(results['Mean_predicted_p_value'] > p_value_threshold, 1, 0)\n",
    "\n",
    "    precision = precision_score(results['Expected'], threshold_specific_prediction)\n",
    "    recall = recall_score(results['Expected'], threshold_specific_prediction)\n",
    "\n",
    "    precision_recall_current = pd.DataFrame([[precision, recall]], \n",
    "                                            columns=column_names\n",
    "                                           )\n",
    "    precision_recall = pd.concat([precision_recall, precision_recall_current],\n",
    "                                ignore_index=True)\n",
    "\n",
    "    \n",
    "# Plot results\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.lineplot(data=precision_recall, x='Recall', y='Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlim(0, 1.05)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.lineplot(data=precision_recall, x='Recall', y='Precision')\n",
    "plt.title('Precision-Recall Curve (autoscale)')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fde4ae-5efe-4d0d-b304-d5b39b49be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine recall value with 100% precision\n",
    "if precision_recall['Precision'].max() == 1:\n",
    "    best_recall = (precision_recall\n",
    "                       .query('Precision == 1')\n",
    "                       .loc[:, 'Recall']\n",
    "                       .max()\n",
    "                  )\n",
    "else:\n",
    "    print('Precision never reaches 1')\n",
    "    \n",
    "print(f'100% Precision with Recall of {round(best_recall * 100, 1)}%')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae2d70-4a55-42ad-9007-910fa3ea9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine results with metadata\n",
    "results = pd.merge(results, metadata, how='left', on='Accession')\n",
    "results = results.loc[:, ['Accession',\n",
    "                            'Cell_line',\n",
    "                            'Diff_efficiency',\n",
    "                            'Jerber_model_score',\n",
    "                            'Mean_predicted_p_value',\n",
    "                            'STD_predicted_p_value',\n",
    "                            'Expected',\n",
    "                            'Predicted',\n",
    "                            'Accuracy']\n",
    "                       ]                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0529595-11e7-4cd5-894c-1974f1c24a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot scatterplot of results vs expected\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.scatterplot(x=\"Diff_efficiency\", \n",
    "            y=\"Mean_predicted_p_value\",\n",
    "            hue='Accuracy',\n",
    "            data=results)\n",
    "\n",
    "plt.title('Classifier p-values vs differetiation efficiency')\n",
    "plt.xlabel('Differentiation Efficiency')\n",
    "plt.ylabel('Mean predicted p value')\n",
    "plt.axhline(0.5, color='r', linestyle='--')\n",
    "plt.axvline(0.2, color='r', linestyle='--')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Put the legend out of the figure\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d387617-0ed0-4219-9494-66fc9a3b8193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with Jerber results\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.color_palette(\"dark\")\n",
    "sns.scatterplot(x=\"Jerber_model_score\", \n",
    "            y=\"Mean_predicted_p_value\",\n",
    "            hue='Accuracy',\n",
    "            data=results)\n",
    "\n",
    "plt.title('Classifier p-values vs Jerber Model Score')\n",
    "plt.xlabel('Jerber Model Score')\n",
    "plt.ylabel('Mean predicted p value')\n",
    "plt.axhline(0.5, color='r', linestyle='--')\n",
    "plt.axvline(0.2, color='r', linestyle='--')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Put the legend out of the figure\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb3f85-434d-438c-909e-60c66c620a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show standard deviation\n",
    "\n",
    "#Plot scatterplot of results vs expected\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.color_palette(\"dark\")\n",
    "sns.scatterplot(x=\"STD_predicted_p_value\", \n",
    "            y=\"Mean_predicted_p_value\",\n",
    "            hue='Accuracy',\n",
    "            data=results)\n",
    "\n",
    "plt.title('Classifier p-values vs standard deviation')\n",
    "plt.xlabel('Standard deviation predicted p value')\n",
    "plt.ylabel('Mean predicted p value')\n",
    "plt.axhline(0.5, color='r', linestyle='--')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Put the legend out of the figure\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fbecb0-e076-47aa-b5a1-7cc3ba385813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out results\n",
    "outfile = 'classifier_analysis_summary_results.tsv.gz'\n",
    "print(f'Writing results to: {outfile}')\n",
    "results.to_csv(outfile, index=False, compression='gzip', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2382d5b1-f784-42f6-80c0-46887fc691b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
